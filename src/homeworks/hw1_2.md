# EE274: Homework-1

- **Focus area:** Prefix-free codes, Basic Information theory
- **Due Date:** Oct 20
- **Weightage:** 20%
- **Total Points:** 130
- **Submission Instructions:** Provided at the end of HW (ensure you read these!)
- **Submission link:** 
  - For written part: [HW1-Written]
  - For programming part: [HW1-Code]

The Homework-1 is designed to explore some interesting concepts related to the most basic component of most of the real world algorithms, the prefix-free codes. Enjoy!

*Please ensure that you follow the [Stanford Honor Code](https://communitystandards.stanford.edu/policies-guidance/honor-code) while doing the homework. You are encouraged to discuss the homework with your classmates, but you should write your own solutions and code. You are also encouraged to use the internet to look up the syntax of the programming language, but you should not copy-paste code from the internet. If you are unsure about what is allowed, please ask the instructors.*

### Q1: Basic Information theory  (*20 points*)

1. [5 points] Let $X$ be a random variable over positive integers with a distribution 
$$ P_X(X=k) = 2^{-k}, k \geq 1$$ 
Compute the entropy $H(X)$ of random variable $X$. What are the optimal code-lengths for a prefix-free code designed for distribution $P_X$?
2. [5 points] Provide an optimal prefix-free code design for the distribution $P_X$. 
3. [10 points] Let $Y$ be a random variable over positive integers such that $\mathbb{E}(Y) = 2$. 
Then show that:
$$ H(Y) \leq 2 $$ 
For what distribution of the random variable $Y$ does the equality hold, i.e. $H(Y) = 2$? 

### Q2. Uniquely decodable codes (*20 points*)

We mainly explored the prefix-free codes in class. But there is a much broader class of codes, the uniquely decodable codes. In this question, we will show that even with the uniquely decodable codes, we cannot do better than the Entropy. 

1. [5 points] Consider the code `C1` below. Is `C1` uniquely decodable? Implement an decoder for the code below. Briefly justify your decoding procedure. 
    ```
    A -> 10
    B -> 00
    C -> 11
    D -> 110
    ```

The codelengths for the uniquely decodable code are denoted as $l_1, l_2, \ldots, l_{|\mathcal{U}|}$. Also, we deonte the codelength of the n-tuple $u^n$ as $L(u^n)$. In particular, if $u_i = r$ for example, then $L(u_i) = l_r$. 

2. [5 points] Show that $$\left( \sum_{i=1}^{|\mathcal{U}|} 2^{-l_i} \right)^n = \sum_{u^n \in \mathcal{U}^n} 2^{-L(u^n)} $$

3. [5 points] Let $l_{max} = \max_{i=1}^{|\mathcal{U}|} l_i$. Then we can rewrite the summation as: $$\sum_{u^n \in \mathcal{U}^n} 2^{-L(u^n)} = \sum_{j=1}^{n.l_{max}} |\{u^n| L(u^n) = j\}. 2^{-j}$$ (NOTE: Rewriting summation is a common proof trick, and is a useful one to watch out for!). Using `Q5.2` and the identity above, show that: 

$$\left( \sum_{i=1}^{|\mathcal{U}|} 2^{-l_i} \right)^n \leq n.l_{max}$$

4. [5 points] Using `Q5.3` show that uniquely decodable codes satisfy Kraft's inequality. i.e. 

$$\left( \sum_{i=1}^{|\mathcal{U}|} 2^{-l_i} \right) \leq 1$$

NOTE: As Uniquely decodable codes satisfy the Kraft's Inequality, similar to the prefix-free codes proof [ADD LINK], `Q5.4` shows that $H(U)$ also the fundamental limit on the average codelength $\mathbb{E}[L(U)]$. 


### Q3. Prefix free codes (*20 points*)

In practice, quite a few times we don't know the distribution of data beforehand. Let's see how you can apply Huffman coding in practice: 

1. Given the following text file, find what distribution you might want to encode the data with. (hint: the empirical frequencies of the symbols might be helpful). If you use fixed-bitwidth encoder for this data, what is the average codelength?

2. Construct a Huffman code to encode this file, and check if the encode/decode works well. What is the size of your compressed file? 

3. Compute the empirical entropy of your data. How close is the average codelength of your huffman code to entropy? 

4. To make a legitimate compressor, we also need to somehow communicate the distribution to the decoder. Implement code to complete your compressor. What is the overhead due to communicating the "codebook"? 


### Q4: Shannon Code(s) (*30 points*)

1. [5 points] In class we saw one version of the Shannon codes; the [tree-based construction](https://stanforddatacompressionclass.github.io/notes/lossless_iid/prefix_free_codes.html#designing-prefix-free-codes). Let's call it the `ShannonTreeEncoder` and the `ShannonTreeDecoder`. Manually calculate what the codewords should be for the following distributions:

    ```python
    ProbabilityDist({"A": 0.25, "B": 0.25, "C": 0.25, "D": 0.25})
    ProbabilityDist({"A": 0.5, "B": 0.25, "C": 0.12, "D": 0.13})
    ProbabilityDist({"A": 0.9, "B": 0.1})
    ```
    Complete the `ShannonTreeEncoder` implementation in `hw1_q4.py`. NOTE: Make sure that `test_shannon_tree_coding_specific_case()` test passes. 

2. [10 points] Complete the code for the `ShannonTreeDecoder` in `hw1_q4.py`. The `ShannonTreeEncoder` is already implemented for convenience.

---

As a fun exercise, we are going to look at another code construction of the *Shannon code*, which also achieves code-lengths $ l(
symbol) = \left\lceil \log_2 \frac{1}{p(symbol)} \right\rceil $. The construction is actually much simpler than the one
we discussed, but is a bit harder to justify. Here we go:

a. Given the probabilities $p_1, p_2, \ldots, p_k$, sort them in the descending order. WLOG let $$ p_1 \geq p_2 \geq \ldots \geq p_k$$
b. compute the cumulative probabilities $c_1, c_2, \ldots, c_k$ such that:
   $$ \begin{aligned}
   c_1 &= 0 \\
   c_2 &= p_1 \\
   c_3 &= p_1 + p_2 \\
   &\ldots \\
   c_k &= p_1 + p_2 + \ldots + p_{k-1}
   \end{aligned} $$
c. Note that we can represent any real number between $[0,1)$ in binary as $b0.b_1 b_2 b_3 \ldots $, where $b_1, b_2,
   b_3, \ldots$ are some bits. For example:
   $$ \begin{aligned}
   0.5 &= b0.1 \\
   0.25 &= b0.01 \\
   0.3 &= b0.010101...
   \end{aligned} $$ This is very similar to how we represent real numbers using "decimal" floating point value, but it
   is using "binary" floating point values (This is actually similar to how computers represent floating point numbers
   internally!)

d. If the "binary" floating point representation is clear, then the Shannon code for symbol $r$ of codelength $ l_r =
   \left\lceil \log_2 \frac{1}{p_r} \right\rceil $ can be obtained by simply truncating the binary floating point
   representation of $c_r$

The full implementation of this version of Shannon codes can be found in SCL [Shannon codes](https://github.com/kedartatwawadi/stanford_compression_library/blob/dfb15ecaa7ac060180531146c25a803d20bf8d8f/compressors/shannon_coder.py)

This version of Shannon codes is much simpler that the Tree based construction we have. But, it is not obvious why it works. i.e. why should the construction lead to prefix-free codes.

3. [5 points] Given a binary string, $x = b_1 b_2 \ldots b_k$, let $R(x)$ be the real number $b0.b_1b_2b_3 \ldots b_k$ corresponding to it lying between $[0,1)$. For example: `10110 -> b0.10110`. Show that, a binary string $y$ has $x$ as its prefix, if and only if:

    $$R(x) \leq R(y) < R(x) + 2^{-k}$$

4. [5 points] Using the property in `Q4.3` prove that any Prefix-free code with lengths $l_1,l_2,\ldots,l_k$, satisfies:
$$ \sum_{i=1}^n 2^{-l_i} \leq 1 $$
i.e. we have an alternative proof for the Kraft inequality. 

5. [5 points] Using the property in `part 3` show that the construction of the Shannon codes does indeed lead to prefix-free codes. 



### Q5: Camping Trip (*20 points*)

During one of the camping trips, Pulkit was given $n$ rope segments of lengths $l_1, l_2,\ldots, l_n$ by Kedar, and was asked to join all the ropes into a single long segment of length $\sum_{i=1}^n l_i$. Pulkit can only join two ropes at a time and the "effort" in joining ropes $i, j$ using a knot is $l_i + l_j$. Pulkit is lazy and would like to minimize his "effort" to join all the segments together.  

1. [5 points] Do you see any parallels with the problem and one of the prefix-free codes you have learnt? Please justify your answer.
2. [5 points] Let $E_{opt}$ optimal value for the effort. Without solving the problem, can Pulkit get an estimate of what the $E_{opt}$ would be? @kedar: are u expecting students to say Entropy here? Else how do u estimate Huffman Code length without building the tree? 
3. [10 points] Implement the function `compute_minimum_effort()` in the file `hw1_q5.py`.  
HINT: You may use one of the prefix-free code implementations in the SCL Library  


### Q6: Typicality-based lossless compression experiment (*15 points*)

In this problem we will analyze the lossless compressor based on the concept of typical sets and the Asymptotic Equipartition Property (AEP) (the one we saw/will see in class). Recall that for a given iid source $X_1, X_2,...$ i.i.d. $\sim X$ (from alphabet $\mathcal{X}$), and for given values of $n > 0$ and $\epsilon > 0$, the typical set is defined as 
$$A_{\epsilon}^{(n)} = \left\{x^n \mid \left\Vert-\frac{1}{n}\log P(x^n) - H(X)\right\Vert \leq \epsilon\right\}$$

We work with the simple coding scheme on chunks of length $n$:
- if $x^n \in A_{\epsilon}^{(n)}$, encode as a $0$ followed by a $\lceil\log_2|A_{\epsilon}^{(n)}|\rceil$ bit representation of the index of $x^n$ in the typical set.
- else, encode as $1$ followed by a $\lceil\log_2|\mathcal{X}|\rceil$ representation of the index of $x^n$ in $\mathcal{X}^n$.

SCL contains an implementation of this [typicality-based compressor](https://github.com/kedartatwawadi/stanford_compression_library/blob/main/compressors/prefix_free_compressors.py). Feel free to go over it to make sure you understand the encoding and decoding procedure. In this problem, we will try to understand this *typicality-based compressor*. 


1. [3 points] For a fixed small $\epsilon$, as $n$ increases, what is the limit of the expected code length per symbol $\frac{1}{n}E[l(X^n)]$.

2. [3 points] What is the computational complexity for this compressor? We are interested in how the encoding/decoding time and memory usage changes as $n$ grows. Is this code practically useful?

Lets try to experiment with the *Typicality-based compressor*. We provide you with a script `hw1_q6.py` to run this encoder on a few different Bernoulli sources with varying values of $\epsilon$ and $n$. Please run the script to generate the three plots that are named like `typical_code_testing_ber_*.png`. Feel free to modify the script and run with other parameters if that helps with the questions below.


*WARNING: The encoding gets slower with $n$, and depending on your machine configuration it might take 10-15 minutes or more for running the script. The script prints the current $n$ and $\epsilon$ being tested so you can monitor the progress. If it is running very slowly, you can consider reducing the value of `max_n` slightly. If you have a fast machine, (or if you are feeling adventurous) you can increase $n$ slightly!*


3. [3 points] In the plots, does the typical encoder do better than the entropy for the uniform distribution? Does it approach the entropy of the actual Bernoulli distribution? How does your answer depend on the value of $\epsilon$?

4. [3 points] Explain the results when $\epsilon$ is set to $100$.  

5. [3 points] Explain the results when $\epsilon$ is set to $0.01$. What would you expect to happen as we increase $n$ further?  


*NOTE: Observe that this code is fixed-length for the most part (except when the input is non-typical). Despite the increased complexity, there are certain scenarios such as hardware/circuit implementations where the higher certainty about codeword lengths makes this code preferable over variable length schemes like Huffman coding.*


### Q7: HW1 Feedback *(5 points)* 
Please answer the following questions, so that we can adjust the difficulty/nature of the problems for the next HWs.

1. How much time did you spent on the HW in total?
2. Which question did you enjoy the most? 
3. Is the programming components in the HWs help you understand the concepts better?
4. Did the HW questions complement the lectures?
5. Any other comments?

### Submission Instructions
Please submit both the written part and your code on Gradescope in their respective submission links. **We will be using both autograder and manual code evaluation for evaluating the coding parts of the assignments.** You can see the scores of the autograded part of the submissions immediately. For code submission ensure following steps are followed for autograder to work correctly:

- As with HW0, you only need to submit the modified files as mentioned in the problem statement.
- Compress the `HW1` folder into a zip file. One way to obtain this zip file is by running the following zip command in the `HWs` folder, i.e.
  ```sh
  cd HWs
  zip -r HW1.zip HW1
  ```
  Note: To avoid autograder errors, ensure that the directory structure is maintained and that you have compressed `HW1` folder containing the relevant files and not `HWs` folder, or files inside or something else. Ensure the name of the files inside the folder are exactly as provided in the starter code, i.e. `hw1_p4.py`, `hw1_p5.py` etc.. In summary, your zip file should be uncompressed to following directory structure (with same names):
  ```
  HW1
  ├── hw1_p3.py
  ├── hw1_p4.py
  ├── hw1_p5.py
  └── hw1_p6.py
  ```
  
- Submit the zip file (`HW1.zip` obtained above) on Gradescope Programming Part Submission Link. Ensure that the autograded parts runs and give you correct scores. 

**Before submitting the programming part on Gradescope, we strongly recommend ensuring that the code runs correctly locally.**
